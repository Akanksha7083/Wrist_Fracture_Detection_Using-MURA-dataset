{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akanksha7083/Wrist_Fracture_Detection_Using-MURA-dataset/blob/main/Wrist_Fracture_Detection_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "41pgmBoIiSOn",
        "outputId": "39d807d9-bae8-4710-d45a-15e7130cc2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Install essential libraries\n",
        "!pip install tensorflow keras numpy pandas matplotlib seaborn scikit-learn streamlit -q\n",
        "!pip install -q streamlit_ace\n",
        "\n",
        "# Mount your Google Drive to access uploaded dataset or save models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9Oeuva-irBU",
        "outputId": "13e30953-5319-48f3-ca1c-984ed14d9bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train wrist studies: 3460\n",
            "Valid wrist studies: 237\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/MURA-v1.1/train_labeled_studies.csv', header=None)\n",
        "valid_df = pd.read_csv('/content/drive/MyDrive/MURA-v1.1/valid_labeled_studies.csv', header=None)\n",
        "\n",
        "train_df.columns = ['study_path', 'label']\n",
        "valid_df.columns = ['study_path', 'label']\n",
        "\n",
        "# Filter for only wrist studies\n",
        "train_df = train_df[train_df['study_path'].str.contains('wrist', case=False)]\n",
        "valid_df = valid_df[valid_df['study_path'].str.contains('wrist', case=False)]\n",
        "\n",
        "print(f\"Train wrist studies: {len(train_df)}\")\n",
        "print(f\"Valid wrist studies: {len(valid_df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ICcAH8WujEPz",
        "outputId": "a3e1b93d-fa21-41f5-f89c-f3de133899a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    MURA-v1.1/train/XR_WRIST/patient06359/study1_p...\n",
            "1    MURA-v1.1/train/XR_WRIST/patient06360/study1_p...\n",
            "2    MURA-v1.1/train/XR_WRIST/patient06361/study1_p...\n",
            "3    MURA-v1.1/train/XR_WRIST/patient06362/study1_p...\n",
            "4    MURA-v1.1/train/XR_WRIST/patient06332/study1_p...\n",
            "Name: study_path, dtype: object\n",
            "0    MURA-v1.1/valid/XR_WRIST/patient11185/study1_p...\n",
            "1    MURA-v1.1/valid/XR_WRIST/patient11186/study1_p...\n",
            "2    MURA-v1.1/valid/XR_WRIST/patient11186/study2_p...\n",
            "3    MURA-v1.1/valid/XR_WRIST/patient11186/study3_p...\n",
            "4    MURA-v1.1/valid/XR_WRIST/patient11187/study1_p...\n",
            "Name: study_path, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(train_df['study_path'].head())  # Check the first few 'study_path' values\n",
        "print(valid_df['study_path'].head())  # Check the first few 'study_path' values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5i2SrACjU9H"
      },
      "outputs": [],
      "source": [
        "import os, glob\n",
        "import pandas as pd\n",
        "\n",
        "# âœ… Function to expand study folders into image-level paths\n",
        "def expand_study_paths(df):\n",
        "    images, labels, study_paths = [], [], []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        image_folder = os.path.join('/content/drive/MyDrive', row['study_path'])\n",
        "        image_paths = glob.glob(os.path.join(image_folder, '*.png'))\n",
        "\n",
        "        for path in image_paths:\n",
        "            images.append(path)\n",
        "            labels.append(int(row['label']))\n",
        "            study_paths.append(row['study_path'])\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'image_path': images,\n",
        "        'study_path': study_paths,\n",
        "        'label': labels\n",
        "    })\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo3sx23ikPCc",
        "outputId": "74b62403-68a9-4edf-ca79-a67f9f6e9c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          image_path  \\\n",
            "0  /content/drive/MyDrive/MURA-v1.1/train/XR_WRIS...   \n",
            "1  /content/drive/MyDrive/MURA-v1.1/train/XR_WRIS...   \n",
            "2  /content/drive/MyDrive/MURA-v1.1/train/XR_WRIS...   \n",
            "3  /content/drive/MyDrive/MURA-v1.1/train/XR_WRIS...   \n",
            "4  /content/drive/MyDrive/MURA-v1.1/train/XR_WRIS...   \n",
            "\n",
            "                                          study_path  label  \n",
            "0  MURA-v1.1/train/XR_WRIST/patient06359/study1_p...      1  \n",
            "1  MURA-v1.1/train/XR_WRIST/patient06359/study1_p...      1  \n",
            "2  MURA-v1.1/train/XR_WRIST/patient06360/study1_p...      1  \n",
            "3  MURA-v1.1/train/XR_WRIST/patient06360/study1_p...      1  \n",
            "4  MURA-v1.1/train/XR_WRIST/patient06360/study1_p...      1  \n",
            "Total train images: 9752, validation images: 659\n"
          ]
        }
      ],
      "source": [
        "# âœ… Expand all study paths into image-level paths\n",
        "train_expanded = expand_study_paths(train_df)\n",
        "valid_expanded = expand_study_paths(valid_df)\n",
        "\n",
        "# ğŸš€ Use the full dataset (no sampling)\n",
        "print(train_expanded.head())\n",
        "print(f\"Total train images: {len(train_expanded)}, validation images: {len(valid_expanded)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "BNuSoeTfiPsY",
        "outputId": "d7f9e0b0-d1c7-4b81-9c2b-dd62dfa1e526"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1378354897.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mwrite_tfrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_expanded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/MURA-v1.1/train_wrist.tfrecords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mwrite_tfrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_expanded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/MURA-v1.1/valid_wrist.tfrecords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1378354897.py\u001b[0m in \u001b[0;36mwrite_tfrecord\u001b[0;34m(df, output_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             example = tf.train.Example(features=tf.train.Features(feature={\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m\"string\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m   \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_io_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    581\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m       return read_file_eager_fallback(\n\u001b[0m\u001b[1;32m    584\u001b[0m           filename, name=name, ctx=_ctx)\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file_eager_fallback\u001b[0;34m(filename, name, ctx)\u001b[0m\n\u001b[1;32m    604\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m   _result = _execute.execute(b\"ReadFile\", 1, inputs=_inputs_flat,\n\u001b[0m\u001b[1;32m    607\u001b[0m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[1;32m    608\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "\n",
        "def write_tfrecord(df, output_path):\n",
        "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with tf.io.TFRecordWriter(output_path) as writer:\n",
        "        for _, row in df.iterrows():\n",
        "            img = tf.io.read_file(row['image_path'])\n",
        "\n",
        "            example = tf.train.Example(features=tf.train.Features(feature={\n",
        "                'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.numpy()])),\n",
        "                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[row['label']]))\n",
        "            }))\n",
        "            writer.write(example.SerializeToString())\n",
        "\n",
        "write_tfrecord(train_expanded, '/content/drive/MyDrive/MURA-v1.1/train_wrist.tfrecords')\n",
        "write_tfrecord(valid_expanded, '/content/drive/MyDrive/MURA-v1.1/valid_wrist.tfrecords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v67mNuUGouDC"
      },
      "outputs": [],
      "source": [
        "def parse_tfrecord(example):\n",
        "    desc = {'image': tf.io.FixedLenFeature([], tf.string), 'label': tf.io.FixedLenFeature([], tf.int64)}\n",
        "    example = tf.io.parse_single_example(example, desc)\n",
        "\n",
        "    image = tf.image.decode_png(example['image'], channels=3)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image = tf.image.per_image_standardization(image)\n",
        "\n",
        "    return image, example['label']\n",
        "\n",
        "def augment_image(image, label):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_brightness(image, 0.2)\n",
        "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
        "    angle = tf.random.uniform([], -0.2*3.14, 0.2*3.14)\n",
        "    image = tf.image.rot90(image, tf.cast(angle // (3.14 / 2), tf.int32))\n",
        "    image = tf.image.resize_with_crop_or_pad(image, 224, 224)\n",
        "    return image, label\n",
        "\n",
        "def load_dataset(path, batch_size=32, shuffle=True, augment=False):\n",
        "    ds = tf.data.TFRecordDataset(path).map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if shuffle: ds = ds.shuffle(1000)\n",
        "    if augment: ds = ds.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset = load_dataset('/content/drive/MyDrive/MURA-v1.1/train_wrist.tfrecords', augment=True)\n",
        "valid_dataset = load_dataset('/content/drive/MyDrive/MURA-v1.1/valid_wrist.tfrecords', shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_-Rsu40rTMX"
      },
      "outputs": [],
      "source": [
        "print(train_df.columns)  # Check column names in train_df\n",
        "print(valid_df.columns)  # Check column names in valid_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5J62g95uqLKO"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Class weight balance\n",
        "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=train_expanded['label'])\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "# Load base model\n",
        "base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze layers\n",
        "for layer in base_model.layers[:150]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Custom classifier head\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.6)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LabG3hB4NGn_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "log_dir = os.path.join(\"logs\", \"fit\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_recall', patience=8, restore_best_weights=True, mode='max'),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6),\n",
        "    ModelCheckpoint('best_model.h5', monitor='val_auc', save_best_only=True, mode='max'),\n",
        "    TensorBoard(log_dir=log_dir)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=50,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=callbacks,\n",
        "    verbose=2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4rnS9WRHjiH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "y_probs = model.predict(valid_dataset).flatten()\n",
        "\n",
        "y_true = []\n",
        "for _, labels in valid_dataset:\n",
        "    y_true.extend(labels.numpy())\n",
        "y_true = np.array(y_true)\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
        "\n",
        "precision_target = 0.8\n",
        "meeting = np.where(precision[:-1] >= precision_target)[0]\n",
        "optimal_idx = meeting[0] if len(meeting) > 0 else np.argmax(2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-9))\n",
        "optimal_threshold = thresholds[optimal_idx] if len(thresholds) > 0 else 0.5\n",
        "\n",
        "y_pred = (y_probs > optimal_threshold).astype(int)\n",
        "\n",
        "print(\"\\nOptimized Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, zero_division=0))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Fracture', 'Fracture'], yticklabels=['No Fracture', 'Fracture'])\n",
        "plt.title(f'Confusion Matrix (Threshold: {optimal_threshold:.3f})')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label='PR Curve')\n",
        "plt.axvline(x=recall[optimal_idx], color='r', linestyle='--', label=f'Threshold = {optimal_threshold:.3f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvzyeBVvfxUh"
      },
      "outputs": [],
      "source": [
        "print(\"Class counts:\", np.bincount(y_true))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AOpSEPVSPtt"
      },
      "outputs": [],
      "source": [
        "# Example save path inside your Google Drive\n",
        "model_save_path = '/content/drive/MyDrive/MURA-v1.1/wrist_fracture_model.h5'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tnIW7csTSX1c"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit pyngrok tensorflow pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkQseloeSvqc"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Configure app\n",
        "st.set_page_config(page_title=\"Wrist Fracture Detector\", layout=\"centered\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    return tf.keras.models.load_model('/content/drive/MyDrive/MURA-v1.1/wrist_fracture_model.h5')\n",
        "\n",
        "def main():\n",
        "    st.title(\"ğŸ©» Wrist Fracture Detection\")\n",
        "    uploaded_file = st.file_uploader(\"Upload wrist X-ray\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    if uploaded_file:\n",
        "        img = Image.open(uploaded_file).convert('RGB')\n",
        "        st.image(img, caption=\"Uploaded X-ray\", width=300)\n",
        "\n",
        "        model = load_model()\n",
        "        img = img.resize((224, 224))\n",
        "        img_array = np.expand_dims(np.array(img)/255.0, axis=0)\n",
        "\n",
        "        with st.spinner('Analyzing...'):\n",
        "            pred = model.predict(img_array)[0][0]\n",
        "\n",
        "        if pred >  0.35:\n",
        "            st.error(f\" Fracture Detected (Prediction accuracy: {pred:.1%})\")\n",
        "        else:\n",
        "            st.success(f\" No Fracture Detected ((Prediction accuracy: {1-pred:.1%})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "B72XqMr-PaaA"
      },
      "outputs": [],
      "source": [
        "!ngrok config add-authtoken YOUR_NGROK_AUTH_TOKEN_HERE\n",
        "!ngrok config add-authtoken \"2vv8F32qzhZn8AHp0NeBULPjQYT_bwPM2WkvMjGJFbdZJfnJ\"\n",
        "!pip install streamlit tensorflow pillow\n",
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABpX5gtvQS9o"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok streamlit -q\n",
        "\n",
        "import os\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "from threading import Thread\n",
        "\n",
        "# Function to run Streamlit\n",
        "def run_streamlit():\n",
        "    os.system(\"streamlit run app.py --server.port 8501\")\n",
        "\n",
        "# Start Streamlit in background\n",
        "Thread(target=run_streamlit).start()\n",
        "time.sleep(5)\n",
        "\n",
        "# Authenticate ngrok (replace with your own token)\n",
        "ngrok.set_auth_token(\"2vv8F32qzhZn8AHp0NeBULPjQYT_bwPM2WkvMjGJFbdZJfnJ\")\n",
        "\n",
        "# Create tunnel\n",
        "public_url = ngrok.connect(addr=\"8501\", bind_tls=True)\n",
        "print(\"Access your app at:\", public_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DqCJq2N-wK3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOEFVj1NIpe7CntLEd/XfIl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}